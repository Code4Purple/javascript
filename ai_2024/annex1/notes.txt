
Why didn't nerual netoworks never caught on in the 1994?
    Processing power and data was not up to speed to to train models of nerual netoworks.
    Ai was ahead of the processing access back in the 1994.



Comparsion of storage: 

1956
    The massive hardware to store maybe 5mb if you are lucky.

1980 
    Store was a size of a whole pc now, and only got to at that point 10mb

Now 
    The Store is so small and fast the smallest value size carded is a 256GB over the 10mb before hand

Moores Law is why the storage and computers are becoming cheeper over time . This includes processing power as well.

What is Deep Learning ?

Geoffrey Hinton :
    * Grandfather of Deep Learning *
    Deep Learning is the idea is for computers to become on of the best tools for learning which is our brain.
    
    The idea behind the ai :
        Build a artical zone full of nodes and 3 main node layers are input and ouput layers are easy to understand.
        But the hidden layer the infinite processing that the tasks are understood and processed for an output.


The Neuron
    Basic building of artical nerual netoworks.
    The idea is to minic to recreate a brain thought Neurual netoworks.

    Layer 1 : input layer is are ur basic 5 senses 
    Layer 2 : Neuron is the processing center -- hidden layer
    Layer 3 : output layer is the outside processing

The Activation Function
    Threshold Function 1 --> if x >= 0 or 0 if x <= 0
    Sigmoid              --> 1/1+e^-x
    Rectifier            --> x = max(x,0)
    Hyperbolic Tangent   --> 1-e^-2x / 1+e^-2x

How do they learn?
    we update weights as a way for the learning processing for the output to get close to zero error as much
    as possible. 

Grandient Descent
    
Curse of dimensionally 

Forward proagation is calcuting the errors so we can put through the network again through backpropagation.


Steps to take 

Step 1  : Randomly initialise the weights to small numbers close to zero(but not zero)
Step 2  : Input the first observation of your dataset in the input layer, each feature in one input node.
Step 3  : Forward-Proagation: from left to right, the Neurons are activated in a way that the impact of
each Neuron's activation is limited by the weights. Propagate the activations until getting the 
predicted result y.
Step 4  : Compare the predicted result to the actual result. Measure the generated error.
Step 5  : Back-Proagation: from right to left, the error is back-propagated. Update the weights according to 
how much they are responsible for the error. The learning rate decides by how much we update the weights.
Step 6  : Repeat Steps 1 to 5 and update the weights after each observation (Reinforcement Learning). or
          Repeat Steps 1 to 5 and update only after each batch of observations (Batch Learning).
Step 7  : Whent he whole trainning set passed through the ANN, that makes an epoch. Redo more epochs.      